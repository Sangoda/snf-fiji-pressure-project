# pressure project
'''
   ___ _  _ _     ___                                      ___           _           _   
  / __(_)(_|_)   / _ \_ __ ___  ___ ___ _   _ _ __ ___    / _ \_ __ ___ (_) ___  ___| |_ 
 / _\ | || | |  / /_)/ '__/ _ \/ __/ __| | | | '__/ _ \  / /_)/ '__/ _ \| |/ _ \/ __| __|
/ /   | || | | / ___/| | |  __/\__ \__ \ |_| | | |  __/ / ___/| | | (_) | |  __/ (__| |_ 
\/    |_|/ |_| \/    |_|  \___||___/___/\__,_|_|  \___| \/    |_|  \___// |\___|\___|\__|
       |__/                                                           |__/               
                                                                                                          
Data Upload Script by Sanya Gowda                                 
_________________________

This script is designed to automate the process of monitoring and uploading data from the Fiji machine at SNF (Stanford Nanofabrication Facility) to Google Sheets. It continuously monitors a specified folder for new data files generated by the Fiji machine and extracts information from the latest 20 runs. The extracted data, including recipe names and cycles remaining, are then formatted into a table and uploaded to a designated Google Sheets document. Additionally, the script logs any file additions or deletions in the monitored folder and triggers the data upload process when changes are detected.


Functionality Overview:

    - Monitors the "RF Data" folder for changes (file additions or deletions).
    - Extracts data from the latest 20 data files generated by the Fiji machine.
    - Creates a table with information on recipe names and cycles remaining.
    - Logs file additions and deletions in the monitored folder.
    - Uploads the extracted data to a specific Google Sheets document for lab members to access.

Please ensure that you have configured Google API authentication with the provided client secret file and set up the appropriate Google Sheets document before running this script.



NOTES
schedule
5-15 minutes sleep in between if needed otherwise continuous
add a timestamp time and date
new files even when script isn't running
'''

# imports
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import glob
import os
import pygsheets as pyg
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from time import localtime, strftime




# Google API authentication
gc = pyg.authorize(client_secret="../cred/client_secret.json")
print("\nFiji Pressure Project Data Upload Script has started. Waiting for new files.")

def dataToSheets():
    # Open and sort by date RF Data folder 
    folder = "./data/RF Data"
    files_path = os.path.join(folder, '*')
    files = sorted(
        glob.iglob(files_path), key=os.path.getctime, reverse=True) 

    filtered_files = [file for file in files if "00 - STANDBY" not in file]

    # Variables
    recipe_values = []
    cycles_values = []

    # Get the 20 latest files and separate into columns and rows
    for file_path in filtered_files[:20]:
        try:
            data = pd.read_csv(file_path, delimiter="\t")
            data.columns = ["delete", "Time", "For Power (W)", "Refl Power (W)" , "Cycles Remaining", "Recipe", "delete"]
            
            # Extract the first values and append to the lists
            first_recipe = data['Recipe'].iloc[0]
            first_cycles = data['Cycles Remaining'].iloc[0]
            
            recipe_values.append(first_recipe)
            cycles_values.append(first_cycles)
        except pd.errors.EmptyDataError:
            print(f"Empty data in file: {file_path}")
        except pd.errors.ParserError:
            print(f"Failed to parse data in file: {file_path}")
    

    # Create pandas dataframe from Recipe and Cycles data  
    df = pd.DataFrame({
        "Recipe": recipe_values,
        "Cycles": cycles_values
    })

    # Index number starts at 1 instead of 0
    df.index += 1

    # print the table in output then create a csv file to save locally
    print(df)
    file_name = "fijioutput.csv"
    df.to_csv(file_name, sep="\t", index = False)

    # Open the spreadsheet with API by its title or key (replace with your spreadsheet's title or key)
    spreadsheet = gc.open_by_key('18PmSFfngWLYY7hQqkM_0BBNLzPHxi2qM2bQwqUXguXM')

    # Read the CSV file into a pandas DataFrame


    # Select the worksheet where you want to import the data (replace with your worksheet's title or index)
    worksheet = spreadsheet.sheet1

    # Clear existing data
    clear_range = 'D4:G25'
    worksheet.clear(clear_range)

    # Update the worksheet with the DataFrame data
    csv_file_path = "./data/fijioutput.csv"
    worksheet.set_dataframe((df), start='D4')   # Starting cell E4

    print("\nData has been uploaded to Google Sheets.\n")

current_time = strftime("[%Y-%m-%d %H:%M:%S]")

# Define a custom event handler class
class MyHandler(FileSystemEventHandler):
    def on_deleted(self, event):
        if event.is_directory:
            return
        elif event.event_type == 'deleted':
            event.src_file = event.src_path.replace("./data/RF Data", '')
            print(f"\n{current_time} File deleted: {event.src_file}")
            dataToSheets()

    def on_created(self, event):
        if event.is_directory:
            return
        elif event.event_type == 'created':
            event.src_file = event.src_path.replace("./data/RF Data", '')
            print(f"\n{current_time} File added: {event.src_file}")
            dataToSheets()
            

# Create a watchdog observer and attach the custom handler
folder_to_watch = "./data/RF Data"
event_handler = MyHandler()
observer = Observer()
observer.schedule(event_handler, path=folder_to_watch, recursive=False)

# Start the observer to watch for changes
observer.start()

try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    observer.stop()

# Stop the observer when the script is terminated
observer.join()



dataToSheets()